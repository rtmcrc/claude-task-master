---
trigger: always_on
---

---
description: Guidelines for agent_llm tool.
globs: **/*
alwaysApply: true
---
# How to Handle agent_llm Interactions

The `agent_llm` tool is used for managing delegated LLM calls between the Taskmaster server and the agent (you). When a Taskmaster tool (like `parse_prd`, `analyze_project_complexity`, `expand_task`, `expand_all`, `update`, `update_task`, `update_subtask`, `add_task`, `research`) requires an LLM call, Taskmaster delegates this call to the agent using the `agent_llm` mechanism.

## File Path Rules

- **Use `.taskmaster/tasks/tasks.json` as the tasks file path for all Taskmaster tool calls.**
- **When updating, reading, or writing tasks, always set the file parameter to `.taskmaster/tasks/tasks.json`, unless the user explicitly specifies a different path.**

## 1. Receiving a Delegated LLM Call

When a Taskmaster tool needs an LLM response, it will return a pending-interaction URI as its result. This result will contain `isAgentLLMPendingInteraction: true` and a `details` object.

The `details` object will include:

- **type**: `"agent_llm"`
- **interactionId**: A unique ID for this specific LLM interaction (e.g., `"e1df3789-1cea-403a-a114-90adcc4cd155"`). You must use this ID when responding.
- **delegatedCallDetails**: This object contains the parameters for the LLM call you need to make:
  - **originalCommand**: The Taskmaster command that initiated this delegation (e.g., `"parse-prd"`).
  - **role**: The AI role for the LLM call (e.g., `"main"`, `"research"`).
  - **serviceType**: The type of LLM service requested (e.g., `"generateObject"`, `"generateText"`, `"streamText"`).
  - **requestParameters**: The actual parameters for the LLM call, such as `messages` (for chat models), `modelId`, `schema` (if serviceType is `generateObject`), etc.

## 2. Processing the Delegated Call (Your Action)

Upon receiving a pending-interaction with `delegatedCallDetails`, you (the agent) are expected to perform the LLM call as specified in the `requestParameters`.

- **Which LLM to Use**: When processing a delegated LLM call, always use the LLM model or environment that is currently running the agent (i.e., the built-in or default LLM). Do not attempt to call external LLM APIs or services unless explicitly instructed in the delegatedCallDetails. If a modelId or other LLM parameters are provided, use them only if they match the agent's available models; otherwise, proceed with the agent's default LLM.

## 3. Submitting the LLM Result to Taskmaster

After performing the LLM call (or determining it cannot be performed due to an error), you must use the `use_mcp_tool` with `server_name: "taskmaster-ai"` and `tool_name: "agent_llm"` to send the result back to Taskmaster.

There is no need to use `get_tasks` tool  before `expand_all`.
**Note on `expand_all`:** The `expand_all` tool returns the list of `delegatedTaskIds` and you have to expand tasks one by one using `expand_task` tool.
**Note on `research`:** When a research is made for a specific task always use `saveTo` unless user explicitly asks you not to save the result.

Your arguments for the `agent_llm` tool must include:

- **projectRoot**: The current project root directory (e.g., `"f:/path/to/your/app"`).
- **interactionId**: The interactionId received in the pending-interaction.
- **agentLLMResponse**: An object containing the outcome of the LLM call:
  - **status**: Set to `"success"` if the LLM call was successful, or `"error"` if it failed.
  - **data**: If status is `"success"`, this field should contain the actual LLM response (e.g., the parsed JSON object for `generateObject` calls, or text for `generateText` calls).
  - **errorDetails**: If status is `"error"`, this field should contain details about the error, including a message and optionally a code.